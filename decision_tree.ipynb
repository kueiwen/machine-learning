{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree is like a flow chart with decision, arrow and terminal. The terminal is the class for classification and predicted value for regression. \n",
    "<img src=\"img/decision_tree.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One key for the decision tree is how to decide a good split point. With a good split, the groups after sjplitting should have small intra-differnce and relative large inter-difference.\n",
    "Here two index can measure the randomness of data: **entropy** and **gini**\n",
    "  \n",
    "<img src=\"img/decision_tree_entropy.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **entropy**\n",
    "$$H(x)=-\\Sigma_{i}^{n}P(x_i)\\text{log}_{2}P(x_i)$$\n",
    "\n",
    "$\\Sigma_{i}^{n}$ is the pbability of event, $\\text{log}_{2}P(x_i)$ represents the amount of information coming from the event.\n",
    "\n",
    "As there is the minus sign, so the lower the entropy, the more information delivered.\n",
    "\n",
    "\n",
    "To proof this, assumed that there are 2 groups A abd B, obviously can see that A group has higher purity than B.\n",
    "\n",
    "Let's calculate the entropy for these two groups.  \n",
    "  \n",
    "$H_A = \\frac{24}{30}\\text{log}_{2}\\frac{24}{30} + \\frac{6}{30}\\text{log}_{2}\\frac{6}{30}=0.72$\n",
    "  \n",
    "$H_B = \\frac{15}{30}\\text{log}_{2}\\frac{15}{30} + \\frac{15}{30}\\text{log}_{2}\\frac{15}{30}=1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **gini**\n",
    "$$G(x) = 1 - \\Sigma_{i}^{n}P(x_i)^2$$\n",
    "  \n",
    "Let use the same example group A anf group B to calculate gini.\n",
    "\n",
    "$$G_A = 1 - ((\\frac{24}{30})^2+(\\frac{6}{30})^2) = 0.32$$\n",
    "$$G_B = 1 - ((\\frac{15}{30})^2+(\\frac{15}{30})^2) = 0.5$$\n",
    "\n",
    "The same as entropy, A has lower value than B, so the lower the gini, the higher the purity.\n",
    " \n",
    "As a result, we need to find a split can get lower entropy or entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# load iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X=np.array(iris.data)\n",
    "y = np.array(iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(class_probabilitues: list) -> float:\n",
    "    \"\"\"Implement the entropy function\n",
    "    Args:\n",
    "        class_probabilitues (list): A list of class probabilities\n",
    "    Returns:\n",
    "        entropy (float): The entropy of the given class probabilities\"\"\"\n",
    "    return sum([-p * np.log2(p+1e-10) for p in class_probabilitues])\n",
    "\n",
    "def gini(class_probabilities: list) -> float:\n",
    "    \"\"\"Implement the gini function\n",
    "    Args:\n",
    "        class_probabilities (list): A list of class probabilities\n",
    "    Returns:\n",
    "        gini (float): The gini of the given class probabilities\"\"\"\n",
    "    return 1 - sum([p**2 for p in class_probabilities])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "def split(X: np.array, y: np.array, feature_idx: int, feature_val: float) -> Tuple[np.array, np.array]:\n",
    "    \"\"\"Split the data into two group with the given feature and value\n",
    "    Args:\n",
    "        X (np.array): The input data\n",
    "        y (np.array): The target labels\n",
    "        feature_idx (int): The index of the feature to split\n",
    "        feature_val (float): The value to split on\n",
    "    Returns:\n",
    "        x1 (np.array): left data\n",
    "        x2 (np.array): right data\n",
    "        y1 (np.array): left labels\n",
    "        y2 (np.array): right labels\n",
    "        p1 (np.array): The probability of each class in the left split data\n",
    "        p2 (np.array): The probability of each class in the right split data\"\"\"\n",
    "    x1 = X[X.T[feature_idx]<feature_val]\n",
    "    x2 = X[X.T[feature_idx]>=feature_val]\n",
    "    y1 = y[X.T[feature_idx]<feature_val]\n",
    "    y2 = y[X.T[feature_idx]>=feature_val]\n",
    "    n_classes = len(np.unique(y))\n",
    "\n",
    "    if len(y1) == 0:\n",
    "        p1 = [0]*n_classes\n",
    "    else:\n",
    "        p1 = [np.sum(y1==c)/len(y1) for c in range(n_classes)]\n",
    "    if len(y2) == 0:\n",
    "        p2 = [0]*n_classes\n",
    "    else:\n",
    "        p2 = [np.sum(y2==c)/len(y2) for c in range(n_classes)]\n",
    "\n",
    "    return x1, x2, y1, y2, p1, p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_split(X, y, criterion):\n",
    "    \"\"\"Find the best split for the given data and criterion\n",
    "    Args:\n",
    "        X (np.array): The input data\n",
    "        y (np.array): The target labels\n",
    "        criterion (str): The criterion to use for finding the best split\n",
    "    Returns:\n",
    "        best_split (dict): The best split found by the algorithm\"\"\"\n",
    "    n_features = X.shape[1]\n",
    "    best_score = 10e+10\n",
    "    flag = False\n",
    "    for feature_idx in range(n_features):\n",
    "        for feature_val in np.unique(X.T[feature_idx]):\n",
    "            x1, x2, y1, y2, p1, p2 = split(X, y, feature_idx, feature_val)\n",
    "            current_split = {'feature_idx': feature_idx, 'feature_val': feature_val, \n",
    "                              'x1': x1, 'x2': x2, 'y1': y1, 'y2': y2, 'best_score': best_score, 'p1': p1, 'p2': p2}\n",
    "            if criterion == 'entropy':\n",
    "                score = entropy(p1) + entropy(p2)\n",
    "            elif criterion == 'gini':\n",
    "                score = gini(p1) + gini(p2)\n",
    "            if score < best_score:\n",
    "                flag = True\n",
    "                best_score = score\n",
    "                best_split = current_split\n",
    "    if flag:\n",
    "        return best_split['feature_idx'], best_split['feature_val'], best_split['best_score'], \\\n",
    "                best_split['x1'], best_split['x2'], best_split['y1'], best_split['y2']\n",
    "    else:\n",
    "        return current_split['feature_idx'], current_split['feature_val'], current_split['best_score'], \\\n",
    "                current_split['x1'], current_split['x2'], current_split['y1'], current_split['y2']\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from treenode import TreeNode\n",
    "total_nclasses = len(np.unique(y))\n",
    "class TreeNode:\n",
    "    def __init__(self, X, feature_idx, feature_val, label_probs, information_gain, parent = None):\n",
    "        self.X = X\n",
    "        self.feature_idx: int = feature_idx\n",
    "        self.feature_val: float = feature_val\n",
    "        self.label_probs: list = label_probs\n",
    "        self.information_gain: float = information_gain\n",
    "        self.parent: TreeNode = parent\n",
    "        self.left: TreeNode = None\n",
    "        self.right: TreeNode = None\n",
    "    \n",
    "    def update_left(self, left):\n",
    "        self.left = left\n",
    "\n",
    "    def update_right(self, right):\n",
    "        self.right = right\n",
    "\n",
    "def get_data_prob(y):\n",
    "    \"\"\"Get the probability of each class in the given data\n",
    "    Args:\n",
    "        y (np.array): The target labels\n",
    "    Returns:\n",
    "        label_probs (np.array): The probability of each class in the given data\"\"\"\n",
    "    label_probs = np.zeros(total_nclasses, dtype = float)\n",
    "    for label in range(total_nclasses):\n",
    "        label_probs[label] = np.sum(y == label) / len(y)\n",
    "    return label_probs\n",
    "\n",
    "\n",
    "def build_tree(X, y, criterion, current_depth, max_depth=6, min_samples_leaf=1, parent = None):\n",
    "    \"\"\"Build a decision tree for the given data\n",
    "    Args:\n",
    "        X (np.array): The input data\n",
    "        criterion (str): The criterion to use for finding the best split\n",
    "        max_depth (int): The maximum depth of the tree\n",
    "    Returns:\n",
    "        root (TreeNode): The root node of the decision tree\"\"\"\n",
    "    \n",
    "    if current_depth > max_depth:\n",
    "        return None\n",
    "    \n",
    "    feature_idx, feature_val, best_score, x1, x2, y1, y2 = find_best_split(X, y, criterion)\n",
    "    label_probs = get_data_prob(y)\n",
    "    if criterion == 'entropy':\n",
    "        node_info = entropy(label_probs)\n",
    "    elif criterion == 'gini':\n",
    "        node_info = gini(label_probs)\n",
    "    information_gain = node_info - best_score\n",
    "    node = TreeNode(X, feature_idx, feature_val, label_probs, information_gain, parent)\n",
    "    \n",
    "    if len(x1) < min_samples_leaf or len(x2) < min_samples_leaf:\n",
    "        return node\n",
    "    #print(current_depth)\n",
    "    current_depth += 1\n",
    "    node.update_left(build_tree(x1, y1, criterion, current_depth, max_depth, parent = node))\n",
    "    node.update_right(build_tree(x2, y2, criterion, current_depth, max_depth, parent = node))\n",
    "\n",
    "    return node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = build_tree(X, y, 'entropy', 0, 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_feature_importance(node):\n",
    "    \"\"\"Calculate the feature importance of the tree\n",
    "    Args:\n",
    "        node (TreeNode): The root node of the tree\n",
    "    Returns:\n",
    "        feature_importances (dict): The feature importance of the tree\n",
    "    \"\"\"\n",
    "    if node is None:\n",
    "        return\n",
    "    if node.left is None and node.right is None:\n",
    "        return\n",
    "    if node.left is not None:\n",
    "        feature_importances[node.feature_idx] += node.information_gain\n",
    "        calculate_feature_importance(node.left)\n",
    "    if node.right is not None:\n",
    "        feature_importances[node.feature_idx] += node.information_gain\n",
    "        calculate_feature_importance(node.right)\n",
    "    return feature_importances\n",
    "feature_importances = dict.fromkeys(range(X.shape[1]), 0)\n",
    "feature_importances = calculate_feature_importance(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2 < 3.0\n",
      " |  - 0 < 4.4\n",
      " |  |  - 0 < 4.3\n",
      "       | - class:  0\n",
      " |  |  - 0 >= 4.3\n",
      "       | - class:  0\n",
      " |  - 0 >= 4.4\n",
      " |  |  - 0 < 4.5\n",
      " |  |  |  - 1 < 3.0\n",
      "          | - class:  0\n",
      " |  |  |  - 1 >= 3.0\n",
      "          | - class:  0\n",
      " |  |  - 0 >= 4.5\n",
      " |  |  |  - 0 < 4.6\n",
      "          | - class:  0\n",
      " |  |  |  - 0 >= 4.6\n",
      "          | - class:  0\n",
      " - 2 >= 3.0\n",
      " |  - 3 < 1.8\n",
      " |  |  - 2 < 5.6\n",
      " |  |  |  - 0 < 4.9\n",
      "          | - class:  1\n",
      " |  |  |  - 0 >= 4.9\n",
      "          | - class:  1\n",
      " |  |  - 2 >= 5.6\n",
      " |  |  |  - 0 < 6.1\n",
      "          | - class:  2\n",
      " |  |  |  - 0 >= 6.1\n",
      "          | - class:  2\n",
      " |  - 3 >= 1.8\n",
      " |  |  - 0 < 5.6\n",
      "       | - class:  2\n",
      " |  |  - 0 >= 5.6\n",
      "       | - class:  2\n"
     ]
    }
   ],
   "source": [
    "Node = tree\n",
    "\n",
    "def print_tree(Node, depth = 0):\n",
    "    if Node is None:\n",
    "        return\n",
    "    print(' | '*depth,'-', Node.feature_idx, '<' , Node.feature_val)\n",
    "    if Node.left is None and Node.right is None:\n",
    "        print('   '*depth, '| - class: ', np.argmax(Node.label_probs))\n",
    "    print_tree(Node.left, depth+1)\n",
    "    print(' | '*depth,'-', Node.feature_idx, '>=' , Node.feature_val)\n",
    "    if Node.left is None and Node.right is None:\n",
    "        print('   '*depth, '| - class: ', np.argmax(Node.label_probs))\n",
    "    print_tree(Node.right, depth+1)\n",
    "\n",
    "print_tree(Node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X: np.array) -> np.array:\n",
    "    \"\"\"Predict the class of the given input data\n",
    "    Args:\n",
    "        X (np.array): The input data\n",
    "    Returns:\n",
    "        predictions (np.array): The predicted class of the input data\"\"\"\n",
    "    predictions = []\n",
    "    for x in X:\n",
    "        node = tree\n",
    "        while node.left is not None and node.right is not None:\n",
    "            if x[node.feature_idx] < node.feature_val:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        predictions.append(np.argmax(node.label_probs))\n",
    "    return np.array(predictions)\n",
    "y_pred = predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97 %\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(y, y_pred)\n",
    "\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\": 10})\n",
    "\n",
    "\n",
    "acc = np.sum(y==y_pred)/len(y_pred)\n",
    "print(\"Accuracy: {} %\".format(round(acc*100),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 93 %\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "tree = build_tree(X_train, y_train, 'entropy', 0, 5, 3)\n",
    "y_pred = predict(X_test)\n",
    "\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\": 10})\n",
    "\n",
    "\n",
    "acc = np.sum(y_test==y_pred)/len(y_pred)\n",
    "print(\"Accuracy: {} %\".format(round(acc*100),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class ClassificationTreeNode:\n",
    "    def __init__(self, X, feature_idx, feature_val, label_probs, information_gain, parent = None):\n",
    "        self.X = X\n",
    "        self.feature_idx: int = feature_idx\n",
    "        self.feature_val: float = feature_val\n",
    "        self.label_probs: list = label_probs\n",
    "        self.information_gain: float = information_gain\n",
    "        self.parent: ClassificationTreeNode = parent\n",
    "        self.left: ClassificationTreeNode = None\n",
    "        self.right: ClassificationTreeNode = None\n",
    "    \n",
    "    def update_left(self, left):\n",
    "        self.left = left\n",
    "\n",
    "    def update_right(self, right):\n",
    "        self.right = right\n",
    "\n",
    "class DecisionTreeClassifier:\n",
    "    def __init__(self, criterion: str = 'entropy', max_depth: int = 6, min_samples_leaf: int = 1):\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.tree: DecisionTreeClassifier = None\n",
    "\n",
    "    def update_left(self, left):\n",
    "        self.left = left\n",
    "\n",
    "    def update_right(self, right):\n",
    "        self.right = right\n",
    "\n",
    "    def entropy(self, class_probabilitues: list) -> float:\n",
    "        \"\"\"Implement the entropy function\n",
    "        Args:\n",
    "            class_probabilitues (list): A list of class probabilities\n",
    "        Returns:\n",
    "            entropy (float): The entropy of the given class probabilities\"\"\"\n",
    "        return sum([-p * np.log2(p+1e-10) for p in class_probabilitues])\n",
    "\n",
    "    def gini(self, class_probabilities: list) -> float:\n",
    "        \"\"\"Implement the gini function\n",
    "        Args:\n",
    "            class_probabilities (list): A list of class probabilities\n",
    "        Returns:\n",
    "            gini (float): The gini of the given class probabilities\"\"\"\n",
    "        return 1 - sum([p**2 for p in class_probabilities])\n",
    "    \n",
    "    def split(self, X: np.array, y: np.array, feature_idx: int, feature_val: float) -> Tuple[np.array, np.array]:\n",
    "        \"\"\"Split the data into two group with the given feature and value\n",
    "        Args:\n",
    "            X (np.array): The input data\n",
    "            y (np.array): The target labels\n",
    "            feature_idx (int): The index of the feature to split\n",
    "            feature_val (float): The value to split on\n",
    "        Returns:\n",
    "            x1 (np.array): left data\n",
    "            x2 (np.array): right data\n",
    "            y1 (np.array): left labels\n",
    "            y2 (np.array): right labels\n",
    "            p1 (np.array): The probability of each class in the left split data\n",
    "            p2 (np.array): The probability of each class in the right split data\"\"\"\n",
    "        x1 = X[X.T[feature_idx]<feature_val]\n",
    "        x2 = X[X.T[feature_idx]>=feature_val]\n",
    "        y1 = y[X.T[feature_idx]<feature_val]\n",
    "        y2 = y[X.T[feature_idx]>=feature_val]\n",
    "        n_classes = len(np.unique(y))\n",
    "\n",
    "        if len(y1) == 0:\n",
    "            p1 = [0]*n_classes\n",
    "        else:\n",
    "            p1 = [np.sum(y1==c)/len(y1) for c in range(n_classes)]\n",
    "        if len(y2) == 0:\n",
    "            p2 = [0]*n_classes\n",
    "        else:\n",
    "            p2 = [np.sum(y2==c)/len(y2) for c in range(n_classes)]\n",
    "\n",
    "        return x1, x2, y1, y2, p1, p2\n",
    "\n",
    "    def find_best_split(self, X: np.array, y: np.array) -> Tuple[int, float, float, np.array, np.array, np.array, np.array]:\n",
    "        \"\"\"Find the best split for the given data and criterion\n",
    "        Args:\n",
    "            X (np.array): The input data\n",
    "            y (np.array): The target labels\n",
    "        Returns:\n",
    "            feature_idx (int): The index of the feature to split\n",
    "            feature_val (float): The value to split on\n",
    "            best_score (float): The best score of the split\n",
    "            x1 (np.array): The left split data\n",
    "            x2 (np.array): The right split data\n",
    "            y1 (np.array): The left split target labels\n",
    "            y2 (np.array): The right split target labels\n",
    "            \"\"\"\n",
    "        n_features = X.shape[1]\n",
    "        best_score = 10e+10\n",
    "        flag = False\n",
    "        for feature_idx in range(n_features):\n",
    "            for feature_val in np.unique(X.T[feature_idx]):\n",
    "                x1, x2, y1, y2, p1, p2 = self.split(X, y, feature_idx, feature_val)\n",
    "                current_split = {'feature_idx': feature_idx, 'feature_val': feature_val, \n",
    "                                'x1': x1, 'x2': x2, 'y1': y1, 'y2': y2, 'best_score': best_score, 'p1': p1, 'p2': p2}\n",
    "                if self.criterion == 'entropy':\n",
    "                    score = self.entropy(p1) + self.entropy(p2)\n",
    "                elif self.criterion == 'gini':\n",
    "                    score = self.gini(p1) + self.gini(p2)\n",
    "                if score < best_score:\n",
    "                    flag = True\n",
    "                    best_score = score\n",
    "                    best_split = current_split\n",
    "        if flag:\n",
    "            return best_split['feature_idx'], best_split['feature_val'], best_split['best_score'], \\\n",
    "                    best_split['x1'], best_split['x2'], best_split['y1'], best_split['y2']\n",
    "        else:\n",
    "            return current_split['feature_idx'], current_split['feature_val'], current_split['best_score'], \\\n",
    "                    current_split['x1'], current_split['x2'], current_split['y1'], current_split['y2']\n",
    "            \n",
    "    def get_data_prob(self, y: np.array) -> np.array:\n",
    "        \"\"\"Get the probability of each class in the given data\n",
    "        Args:\n",
    "            y (np.array): The target labels\n",
    "        Returns:\n",
    "            label_probs (np.array): The probability of each class in the given data\"\"\"\n",
    "        label_probs = np.zeros(self.total_nclasses, dtype = float)\n",
    "        for label in range(self.total_nclasses):\n",
    "            label_probs[label] = np.sum(y == label) / len(y)\n",
    "        return label_probs\n",
    "\n",
    "    def fit(self, X: np.array, y: np.array, current_depth: int = 0, parent = None) -> ClassificationTreeNode:\n",
    "        \"\"\"Build a decision tree for the given data\n",
    "        Args:\n",
    "            X (np.array): The input data\n",
    "            max_depth (int): The maximum depth of the tree\n",
    "        Returns:\n",
    "            root (ClassificationTreeNode): The root node of the decision tree\"\"\"\n",
    "        \n",
    "        if current_depth == 0:\n",
    "            self.total_nclasses = len(np.unique(y))\n",
    "\n",
    "        if current_depth > self.max_depth:\n",
    "            return None\n",
    "        \n",
    "        feature_idx, feature_val, best_score, x1, x2, y1, y2 = self.find_best_split(X, y)\n",
    "        label_probs = self.get_data_prob(y)\n",
    "        if self.criterion == 'entropy':\n",
    "            node_info = self.entropy(label_probs)\n",
    "        elif self.criterion == 'gini':\n",
    "            node_info = self.gini(label_probs)\n",
    "        information_gain = node_info - best_score\n",
    "        tree = ClassificationTreeNode(X, feature_idx, feature_val, label_probs, information_gain, parent)\n",
    "\n",
    "        if len(x1) < self.min_samples_leaf or len(x2) < self.min_samples_leaf:\n",
    "            self.tree = tree\n",
    "            return self.tree\n",
    "        #print(current_depth)\n",
    "        current_depth += 1\n",
    "        tree.left = (self.fit(x1, y1, current_depth, parent = tree))\n",
    "        tree.right = (self.fit(x2, y2, current_depth, parent = tree))\n",
    "\n",
    "        self.tree = tree\n",
    "        return self.tree\n",
    "    \n",
    "    def predict(self, X: np.array) -> np.array:\n",
    "        \"\"\"Predict the class of the given input data\n",
    "        Args:\n",
    "            X (np.array): The input data\n",
    "        Returns:\n",
    "            predictions (np.array): The predicted class of the input data\"\"\"\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            node = self.tree\n",
    "            while node.left is not None and node.right is not None:\n",
    "                if x[node.feature_idx] < node.feature_val:\n",
    "                    node = node.left\n",
    "                else:\n",
    "                    node = node.right\n",
    "            predictions.append(np.argmax(node.label_probs))\n",
    "        return np.array(predictions)\n",
    "    \n",
    "    def print_tree(self, Node, depth = 0):\n",
    "        if depth == 0:\n",
    "            Node = self.tree\n",
    "        if Node is None:\n",
    "            return\n",
    "        print(' | '*depth,'-', Node.feature_idx, '<' , Node.feature_val)\n",
    "        if Node.left is None and Node.right is None:\n",
    "            print('   '*depth, '| - class :', np.argmax(Node.label_probs))\n",
    "        self.print_tree(Node.left, depth+1)\n",
    "        print(' | '*depth,'-', Node.feature_idx, '>=' , Node.feature_val)\n",
    "        if Node.left is None and Node.right is None:\n",
    "            print('   '*depth, '| - class: ', np.argmax(Node.label_probs))\n",
    "        self.print_tree(Node.right, depth+1)\n",
    "\n",
    "    def calculate_feature_importance(self, node):\n",
    "        \"\"\"Calculate the feature importance of the tree\n",
    "        Args:\n",
    "            node (TreeNode): The root node of the tree\n",
    "        Returns:\n",
    "            feature_importances (dict): The feature importance of the tree\n",
    "        \"\"\"\n",
    "        if node is None:\n",
    "            return\n",
    "        if node.left is None and node.right is None:\n",
    "            return\n",
    "        if node.left is not None:\n",
    "            self.feature_importances[node.feature_idx] += node.information_gain\n",
    "            self.calculate_feature_importance(node.left)\n",
    "        if node.right is not None:\n",
    "            self.feature_importances[node.feature_idx] += node.information_gain\n",
    "            self.calculate_feature_importance(node.right)\n",
    "        return self.feature_importances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2 < 3.3\n",
      " |  - 0 < 4.4\n",
      " |  |  - 0 < 4.3\n",
      "       | - class : 0\n",
      " |  |  - 0 >= 4.3\n",
      "       | - class:  0\n",
      " |  - 0 >= 4.4\n",
      " |  |  - 0 < 4.5\n",
      " |  |  |  - 1 < 3.0\n",
      "          | - class : 0\n",
      " |  |  |  - 1 >= 3.0\n",
      "          | - class:  0\n",
      " |  |  - 0 >= 4.5\n",
      " |  |  |  - 0 < 4.6\n",
      "          | - class : 0\n",
      " |  |  |  - 0 >= 4.6\n",
      "          | - class:  0\n",
      " - 2 >= 3.3\n",
      " |  - 3 < 1.7\n",
      " |  |  - 2 < 5.6\n",
      " |  |  |  - 0 < 4.9\n",
      "          | - class : 1\n",
      " |  |  |  - 0 >= 4.9\n",
      "          | - class:  1\n",
      " |  |  - 2 >= 5.6\n",
      " |  |  |  - 0 < 6.1\n",
      "          | - class : 2\n",
      " |  |  |  - 0 >= 6.1\n",
      "          | - class:  2\n",
      " |  - 3 >= 1.7\n",
      " |  |  - 0 < 4.9\n",
      "       | - class : 2\n",
      " |  |  - 0 >= 4.9\n",
      "       | - class:  2\n",
      "Accuracy: 93 %\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "\n",
    "# load iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X=np.array(iris.data)\n",
    "y = np.array(iris.target)\n",
    "\n",
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Build up the decision tree classifier\n",
    "dt = DecisionTreeClassifier(max_depth = 3)\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Predict on testing data\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "# Print out the result\n",
    "dt.print_tree(dt.tree)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\": 10})\n",
    "\n",
    "\n",
    "acc = np.sum(y_test==y_pred)/len(y_pred)\n",
    "print(\"Accuracy: {} %\".format(round(acc*100),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree can also be apply to regression probelms, the differnce is that MSE is used to determine the best split,\n",
    "\n",
    "$$MSE = \\frac{1}{n} \\Sigma_i^n {(y_i-\\overline{y})^2}$$\n",
    "\n",
    "where $\\overline{y}$ is the mean of y in the group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class RegressionTreeNode:\n",
    "    def __init__(self, X, feature_idx, feature_val, parent = None):\n",
    "        self.X = X\n",
    "        self.feature_idx: int = feature_idx\n",
    "        self.feature_val: float = feature_val\n",
    "        self.parent: RegressionTreeNode = parent\n",
    "        self.left: RegressionTreeNode = None\n",
    "        self.right: RegressionTreeNode = None\n",
    "        self.predict_val: float = None\n",
    "    \n",
    "    def update_left(self, left):\n",
    "        self.left = left\n",
    "\n",
    "    def update_right(self, right):\n",
    "        self.right = right\n",
    "\n",
    "class DecisionTreeRegressor:\n",
    "    def __init__(self, max_depth: int = 6, min_samples_leaf: int = 1):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.tree: DecisionTreeRegressor = None\n",
    "\n",
    "    def mse(self, y_left: np.array, y_right: np.array) -> Tuple[float, float]:\n",
    "        \"\"\"Implement the mse function\n",
    "        Args:\n",
    "            y_left (np.array): An array of y values in left group\n",
    "            y_right (np.array): An array of y values in right group\n",
    "        Returns:\n",
    "            mes_left (float): The mse of the given y_left\n",
    "            mes_right (float): The mse of the given y_right\"\"\"\n",
    "        mes_left = np.mean([(y_left[i]-np.mean(y_left))**2 for i in range(len(y_left))])\n",
    "        mes_right = np.mean([(y_right[i]-np.mean(y_right))**2 for i in range(len(y_right))])\n",
    "        return mes_left, mes_right\n",
    "    \n",
    "    def split(self, X: np.array, y: np.array, feature_idx: int, feature_val: float) -> Tuple[np.array, np.array]:\n",
    "        \"\"\"Split the data into two group with the given feature and value\n",
    "        Args:\n",
    "            X (np.array): The input data\n",
    "            y (np.array): The target labels\n",
    "            feature_idx (int): The index of the feature to split\n",
    "            feature_val (float): The value to split on\n",
    "        Returns:\n",
    "                X_left (np.array): The probability of each class in the left split data\n",
    "                X_right (np.array): The probability of each class in the right split data\"\"\"\n",
    "        x1 = X[X.T[feature_idx]<feature_val]\n",
    "        x2 = X[X.T[feature_idx]>=feature_val]\n",
    "        y1 = y[X.T[feature_idx]<feature_val]\n",
    "        y2 = y[X.T[feature_idx]>=feature_val]\n",
    "\n",
    "        return x1, x2, y1, y2\n",
    "\n",
    "\n",
    "    def get_cost(self, y1, y2):\n",
    "        \"\"\"Calculate the cost of the split\n",
    "        Args:\n",
    "            y1 (np.array): The target labels in the left split\n",
    "            y2 (np.array): The target labels in the right split\n",
    "        Returns:\n",
    "            cost (float): The cost of the split\"\"\"\n",
    "        mse_left, mse_right = self.mse(y1, y2)\n",
    "        return (len(y1)/(len(y1)+len(y2))) * mse_left + (len(y2)/(len(y1)+len(y2))) * mse_right\n",
    "\n",
    "\n",
    "    def find_best_split(self, X: np.array, y: np.array) -> Tuple[int, float, float, np.array, np.array, np.array, np.array]:\n",
    "        \"\"\"Find the best split for the given data and criterion\n",
    "        Args:\n",
    "            X (np.array): The input data\n",
    "            y (np.array): The target labels\n",
    "        Returns:\n",
    "            feature_idx (int): The index of the feature to split\n",
    "            feature_val (float): The value to split on\n",
    "            best_score (float): The best score of the split\n",
    "            x1 (np.array): The left split data\n",
    "            x2 (np.array): The right split data\n",
    "            y1 (np.array): The left split target labels\n",
    "            y2 (np.array): The right split target labels\n",
    "            \"\"\"\n",
    "        n_features = X.shape[1]\n",
    "        best_score = 10e+10\n",
    "        flag = False\n",
    "        for feature_idx in range(n_features):\n",
    "            for feature_val in np.unique(X.T[feature_idx]):\n",
    "                x1, x2, y1, y2 = self.split(X, y, feature_idx, feature_val)\n",
    "                current_split = {'feature_idx': feature_idx, 'feature_val': feature_val, \n",
    "                                'x1': x1, 'x2': x2, 'y1': y1, 'y2': y2, 'best_score': best_score}\n",
    "                score = self.get_cost(y1, y2)\n",
    "                if score < best_score:\n",
    "                    flag = True\n",
    "                    best_score = score\n",
    "                    best_split = current_split\n",
    "        if flag:\n",
    "            return best_split['feature_idx'], best_split['feature_val'], best_split['best_score'], \\\n",
    "                    best_split['x1'], best_split['x2'], best_split['y1'], best_split['y2']\n",
    "        else:\n",
    "            return current_split['feature_idx'], current_split['feature_val'], current_split['best_score'], \\\n",
    "                    current_split['x1'], current_split['x2'], current_split['y1'], current_split['y2']\n",
    "    \n",
    "\n",
    "    def fit(self, X: np.array, y: np.array, current_depth: int = 0, parent = None) -> RegressionTreeNode:\n",
    "        \"\"\"Build a decision tree for the given data\n",
    "        Args:\n",
    "            X (np.array): The input data\n",
    "            max_depth (int): The maximum depth of the tree\n",
    "        Returns:\n",
    "            root (RegressionTreeNode): The root node of the decision tree\"\"\"\n",
    "        \n",
    "        self.total_nclasses = len(np.unique(y))\n",
    "        \n",
    "\n",
    "        if current_depth > self.max_depth:\n",
    "            return None\n",
    "        \n",
    "        feature_idx, feature_val, best_score, x1, x2, y1, y2 = self.find_best_split(X, y)\n",
    "        tree = RegressionTreeNode(X, feature_idx, feature_val, parent)\n",
    "        tree.predict_values = np.mean(y)\n",
    "        \n",
    "        if len(x1) < self.min_samples_leaf or len(x2) < self.min_samples_leaf:\n",
    "            self.tree = tree\n",
    "            return self.tree\n",
    "        #print(current_depth)\n",
    "        current_depth += 1\n",
    "        tree.left = (self.fit(x1, y1, current_depth, parent = tree))\n",
    "        tree.right = (self.fit(x2, y2, current_depth, parent = tree))\n",
    "\n",
    "        self.tree = tree\n",
    "        return self.tree\n",
    "    \n",
    "    def predict(self, X: np.array) -> np.array:\n",
    "        \"\"\"Predict the class of the given input data\n",
    "        Args:\n",
    "            X (np.array): The input data\n",
    "        Returns:\n",
    "            predictions (np.array): The predicted class of the input data\"\"\"\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            node = self.tree\n",
    "            while node.left is not None and node.right is not None:\n",
    "                if x[node.feature_idx] < node.feature_val:\n",
    "                    node = node.left\n",
    "                else:\n",
    "                    node = node.right\n",
    "            predictions.append(node.predict_values)\n",
    "        return np.array(predictions)\n",
    "    \n",
    "    def print_tree(self, Node, depth = 0):\n",
    "        if depth == 0:\n",
    "            Node = self.tree\n",
    "        if Node is None:\n",
    "            return\n",
    "        print(' | '*depth,'-', Node.feature_idx, '<' , Node.predict_values)\n",
    "        if Node.left is None and Node.right is None:\n",
    "            print('   '*depth, '| - value :', np.argmax(Node.predict_values))\n",
    "        self.print_tree(Node.left, depth+1)\n",
    "        print(' | '*depth,'-', Node.feature_idx, '>=' , Node.predict_values)\n",
    "        if Node.left is None and Node.right is None:\n",
    "            print('   '*depth, '| - value: ', np.argmax(Node.predict_values))\n",
    "        self.print_tree(Node.right, depth+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kueiwenchang\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\kueiwenchang\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\core\\_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 8 < 152.13348416289594\n",
      " |  - 2 < 109.9862385321101\n",
      " |  |  - 6 < 96.30994152046783\n",
      " |  |  |  - 4 < 108.80459770114942\n",
      "          | - value : 0\n",
      " |  |  |  - 4 >= 108.80459770114942\n",
      "          | - value:  0\n",
      " |  |  - 6 >= 96.30994152046783\n",
      " |  |  |  - 1 < 83.36904761904762\n",
      "          | - value : 0\n",
      " |  |  |  - 1 >= 83.36904761904762\n",
      "          | - value:  0\n",
      " |  - 2 >= 109.9862385321101\n",
      " |  |  - 0 < 159.74468085106383\n",
      " |  |  |  - 0 < 274.0\n",
      "          | - value : 0\n",
      " |  |  |  - 0 >= 274.0\n",
      "          | - value:  0\n",
      " |  |  - 0 >= 159.74468085106383\n",
      " |  |  |  - 3 < 154.66666666666666\n",
      "          | - value : 0\n",
      " |  |  |  - 3 >= 154.66666666666666\n",
      "          | - value:  0\n",
      " - 8 >= 152.13348416289594\n",
      " |  - 2 < 193.15178571428572\n",
      " |  |  - 2 < 162.68103448275863\n",
      " |  |  |  - 4 < 137.6904761904762\n",
      "          | - value : 0\n",
      " |  |  |  - 4 >= 137.6904761904762\n",
      "          | - value:  0\n",
      " |  |  - 2 >= 162.68103448275863\n",
      " |  |  |  - 8 < 176.86486486486487\n",
      "          | - value : 0\n",
      " |  |  |  - 8 >= 176.86486486486487\n",
      "          | - value:  0\n",
      " |  - 2 >= 193.15178571428572\n",
      " |  |  - 2 < 225.87962962962962\n",
      " |  |  |  - 3 < 208.57142857142858\n",
      "          | - value : 0\n",
      " |  |  |  - 3 >= 208.57142857142858\n",
      "          | - value:  0\n",
      " |  |  - 2 >= 225.87962962962962\n",
      " |  |  |  - 5 < 268.8709677419355\n",
      "          | - value : 0\n",
      " |  |  |  - 5 >= 268.8709677419355\n",
      "          | - value:  0\n",
      "RMSE:  2.588242417183264\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "diabate = load_diabetes()\n",
    "X = diabate.data\n",
    "y = diabate.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "dtr = DecisionTreeRegressor(max_depth = 3)\n",
    "\n",
    "rt = dtr.fit(X, y)\n",
    "\n",
    "dtr.print_tree(rt)\n",
    "\n",
    "y_pred = dtr.predict(X)\n",
    "\n",
    "print('RMSE: ', np.sqrt(np.mean((y - y_pred) ** 2 / len(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reference\n",
    "\n",
    "https://medium.com/@enozeren/building-a-decision-tree-from-scratch-324b9a5ed836\n",
    "\n",
    "https://github.com/zotroneneis/machine_learning_basics/blob/master/decision_tree_regression.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
