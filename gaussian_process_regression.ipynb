{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Process Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous 2 methods for regression problem, we will define a set of parameters and find the best fit combination. However, in some cases it is difficult to know the relation between dependent variables and independent variables, if use parameter-based regression approach, may can not get a effective prediction model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By Bayes rule, we can mapping the uncertainty into a prior over maping and get the posterior. Gaussian process can be used to represent a prior distribution and no parameter is needed.  \n",
    "\n",
    "<img src=\"img/gaussian_process_example.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition**: A Gaussian process is a collection of random variables, any finite number of which have a joint Gaussian distribution, which means each random variable is distributed normally and their joint distribution is also Gaussian.\n",
    "  \n",
    "A Gaussian function has 2 components: mean function $\\mu(x)$ and covariance function $cov(x,x')$\n",
    "\n",
    "$$\\mu(x) = \\Bbb{E}[f(x)]$$\n",
    "$$cov(x,x') = \\Bbb{E}[(f(x)-\\mu(x))(f(x')-\\mu(x'))]$$\n",
    "\n",
    "The Gaussian process can be expressed as,\n",
    "$$f(x)\\sim \\cal{GP}(\\mu(x),cov(x,x'))$$\n",
    "\n",
    "Gaussian distributions have the nice algebraic property of being closed under conditioning and marginalization. Being closed under conditioning and marginalization means that the resulting distributions from these operations are also Gaussian, which makes many problems in statistics and machine learning tractable.\n",
    "\n",
    "If X, Y are tehe subest of a Gaussian process, we can notation as,\n",
    "\n",
    "$$p_{XY} = \\left[\\begin{array}{ccc}X\\\\Y \\end{array}\\right]\\sim \\cal{N}(\\mu, \\Sigma)=\\cal{N}(\n",
    "    \\left[\\begin{array}{ccc}\\mu_X\\\\ \\mu_Y \\end{array}\\right] ,\n",
    "    \\left[\\begin{array}{ccc}\\Sigma_{XX}\\Sigma_{XY}\\\\ \\Sigma_{XY}\\Sigma_{YY} \\end{array}\\right]\n",
    ")$$\n",
    "\n",
    "**Marginalization**  \n",
    "we can dertermine the marginal distributeion of $X$ and $Y$ as,  \n",
    "$X\\sim \\cal{N}(\\mu_x, \\Sigma_{XX})$  \n",
    "$Y\\sim \\cal{N}(\\mu_y, \\Sigma_{YY})$  \n",
    "\n",
    "**Conditioning**  \n",
    "Conditioning is used to determine the probability of one variable depending on another variable. Using the same exampel,  \n",
    "$$X|Y \\sim \\cal{N}(\\mu_x+\\Sigma_{XY}\\Sigma_{YY}^{-1}(Y-\\mu_Y), \\Sigma_{XX}-\\Sigma_{XY}\\Sigma_{YY}^{-1}\\Sigma_{YX})$$\n",
    "$$Y|X \\sim \\cal{N}(\\mu_y+\\Sigma_{YX}\\Sigma_{XX}^{-1}(Y-\\mu_X), \\Sigma_{YY}-\\Sigma_{YX}\\Sigma_{XX}^{-1}\\Sigma_{XY})$$\n",
    "\n",
    "<img src=\"img/gaussian_process_feature.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Bayesian Inference**\n",
    "A Bayesian inference is to update a statistical hypothesis unital the new information untial available. The posterior probability $p(X|Y)$ can be derived from prior probability $p(X)$ and ikelihood function $p(Y|X)$ (can be observed from given data $X$ and $Y$) by Bayesian rule,\n",
    "$$p(X|Y) = {p(Y|X)p(X)} \\over {p(Y)} $$\n",
    "\n",
    "$p(Y)$ is the marginal likelihood, \n",
    "$$p(Y) = \\int p(Y|X)p(X)dX $$\n",
    " \n",
    "We usually take log marginal likelihood $logp(Y)$ as objective function during the learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Gaussian process with Bayesian inference**\n",
    "\n",
    "Here show a two-dimensional gaussian process example.\n",
    "<img src=\"img/gaussian_process_distribution_example.png\" width=\"600\">\n",
    "\n",
    "The ext problem is now to setup the mean $\\mu$ and covariance $\\Sigma$? In Gaussian process covariance matrix is determined by convariance function $k$, called *kernel*.  \n",
    "<img src=\"img/gaussian_process_kernel.png\" width=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reference\n",
    "https://github.com/aidanscannell/probabilistic-modelling/blob/master/notebooks/gaussian-process-regression.ipynb\n",
    "https://distill.pub/2019/visual-exploration-gaussian-processes/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
