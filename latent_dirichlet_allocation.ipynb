{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Latent Dirichlet Allocation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latent Dirichlet Allocation is a kind of topic modelings, which can give each topic of documents in a documentation set a probability distribution. Since label is no need, LDA is unsupervised learning, only documentation set and the number of specific topics are needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA is also a typical bag of word (BAG) model, which assume that a document is consisted of a set of words, and there is no sequence bwtween words. A document can contain multiple topics, and each word is generated by one of topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in conclusion,  LDA can do **feature reduction** (grouping words into topics) and **tagging/labeling** (give topics for each document) for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA is based on Bayesian inference to estimate posterier distribution used likelihood and prior probability.\n",
    "\n",
    "<img src=\"img/latent_dirichlet_allocation.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$k$: the number of topics  \n",
    "$M$: the number of documents in documentation set  \n",
    "$N$: the number of words in a specific document  \n",
    "$V$: vocabulary size  \n",
    "$\\alpha$: Dirichlet prior $D(\\alpha)$ concentration parameter of the per-document topic distribution, whcih with length of $k$  \n",
    "$\\theta_m$: the sampled topic distribution ver $k$ topics for a document $m$ from $D(\\alpha)$  \n",
    "$z_{mn}$: a sampled topic from topic distribution $\\theta_m$ for the $n_th$ word in document $m$\n",
    "$w_{mn}$: the word at position $n$ in document $m$  \n",
    "$\\beta$: a $k\\times V$ matrix where each row represents a Dirichlet prior $D(\\beta_k)$ modeling the word distribution $\\phi_i$ over topic $k$. $\\beta_{kn}=P(w_n|z_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
