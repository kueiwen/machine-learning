{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Decision Tree**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree is like a flow chart with decision, arrow and terminal. The terminal is the class for classification and predicted value for regression. \n",
    "<img src=\"img/decision_tree.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One key for the decision tree is how to decide a good split point. With a good split, the groups after sjplitting should have small intra-differnce and relative large inter-difference.\n",
    "Here two index can measure the randomness of data: **entropy** and **gini**\n",
    "  \n",
    "<img src=\"img/decision_tree_entropy.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **entropy**\n",
    "$$H(x)=-\\Sigma_{i}^{n}P(x_i)\\text{log}_{2}P(x_i)$$\n",
    "\n",
    "$\\Sigma_{i}^{n}$ is the pbability of event, $\\text{log}_{2}P(x_i)$ represents the amount of information coming from the event.\n",
    "\n",
    "As there is the minus sign, so the lower the entropy, the more information delivered.\n",
    "\n",
    "\n",
    "To proof this, assumed that there are 2 groups A abd B, obviously can see that A group has higher purity than B.\n",
    "\n",
    "Let's calculate the entropy for these two groups.  \n",
    "  \n",
    "$H_A = \\frac{24}{30}\\text{log}_{2}\\frac{24}{30} + \\frac{6}{30}\\text{log}_{2}\\frac{6}{30}=0.72$\n",
    "  \n",
    "$H_B = \\frac{15}{30}\\text{log}_{2}\\frac{15}{30} + \\frac{15}{30}\\text{log}_{2}\\frac{15}{30}=1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **gini**\n",
    "$$G(x) = 1 - \\Sigma_{i}^{n}P(x_i)^2$$\n",
    "  \n",
    "Let use the same example group A anf group B to calculate gini.\n",
    "\n",
    "$$G_A = 1 - ((\\frac{24}{30})^2+(\\frac{6}{30})^2) = 0.32$$\n",
    "$$G_B = 1 - ((\\frac{15}{30})^2+(\\frac{15}{30})^2) = 0.5$$\n",
    "\n",
    "The same as entropy, A has lower value than B, so the lower the gini, the higher the purity.\n",
    " \n",
    "As a result, we need to find a split can get lower entropy or entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# load iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X=np.array(iris.data)\n",
    "y = np.array(iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(class_probabilitues: list) -> float:\n",
    "    \"\"\"Implement the entropy function\n",
    "    Args:\n",
    "        class_probabilitues (list): A list of class probabilities\n",
    "    Returns:\n",
    "        entropy (float): The entropy of the given class probabilities\"\"\"\n",
    "    return sum([-p * np.log2(p+1e-10) for p in class_probabilitues])\n",
    "\n",
    "def gini(class_probabilities: list) -> float:\n",
    "    \"\"\"Implement the gini function\n",
    "    Args:\n",
    "        class_probabilities (list): A list of class probabilities\n",
    "    Returns:\n",
    "        gini (float): The gini of the given class probabilities\"\"\"\n",
    "    return 1 - sum([p**2 for p in class_probabilities])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "def split(X: np.array, y: np.array, feature_idx: int, feature_val: float) -> Tuple[np.array, np.array]:\n",
    "    \"\"\"Split the data into two group with the given feature and value\n",
    "    Args:\n",
    "        X (np.array): The input data\n",
    "        y (np.array): The target labels\n",
    "        feature_idx (int): The index of the feature to split\n",
    "        feature_val (float): The value to split on\n",
    "    Returns:\n",
    "        x1 (np.array): left data\n",
    "        x2 (np.array): right data\n",
    "        y1 (np.array): left labels\n",
    "        y2 (np.array): right labels\n",
    "        p1 (np.array): The probability of each class in the left split data\n",
    "        p2 (np.array): The probability of each class in the right split data\"\"\"\n",
    "    x1 = X[X.T[feature_idx]<feature_val]\n",
    "    x2 = X[X.T[feature_idx]>=feature_val]\n",
    "    y1 = y[X.T[feature_idx]<feature_val]\n",
    "    y2 = y[X.T[feature_idx]>=feature_val]\n",
    "    n_classes = len(np.unique(y))\n",
    "\n",
    "    if len(y1) == 0:\n",
    "        p1 = [0]*n_classes\n",
    "    else:\n",
    "        p1 = [np.sum(y1==c)/len(y1) for c in range(n_classes)]\n",
    "    if len(y2) == 0:\n",
    "        p2 = [0]*n_classes\n",
    "    else:\n",
    "        p2 = [np.sum(y2==c)/len(y2) for c in range(n_classes)]\n",
    "\n",
    "    return x1, x2, y1, y2, p1, p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_split(X, y, criterion):\n",
    "    \"\"\"Find the best split for the given data and criterion\n",
    "    Args:\n",
    "        X (np.array): The input data\n",
    "        y (np.array): The target labels\n",
    "        criterion (str): The criterion to use for finding the best split\n",
    "    Returns:\n",
    "        best_split (dict): The best split found by the algorithm\"\"\"\n",
    "    n_features = X.shape[1]\n",
    "    best_score = 10e+10\n",
    "    flag = False\n",
    "    for feature_idx in range(n_features):\n",
    "        for feature_val in np.unique(X.T[feature_idx]):\n",
    "            x1, x2, y1, y2, p1, p2 = split(X, y, feature_idx, feature_val)\n",
    "            current_split = {'feature_idx': feature_idx, 'feature_val': feature_val, \n",
    "                              'x1': x1, 'x2': x2, 'y1': y1, 'y2': y2, 'best_score': best_score, 'p1': p1, 'p2': p2}\n",
    "            if criterion == 'entropy':\n",
    "                score = entropy(p1) + entropy(p2)\n",
    "            elif criterion == 'gini':\n",
    "                score = gini(p1) + gini(p2)\n",
    "            if score < best_score:\n",
    "                flag = True\n",
    "                best_score = score\n",
    "                best_split = current_split\n",
    "    if flag:\n",
    "        return best_split['feature_idx'], best_split['feature_val'], best_split['best_score'], \\\n",
    "                best_split['x1'], best_split['x2'], best_split['y1'], best_split['y2']\n",
    "    else:\n",
    "        return current_split['feature_idx'], current_split['feature_val'], current_split['best_score'], \\\n",
    "                current_split['x1'], current_split['x2'], current_split['y1'], current_split['y2']\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from treenode import TreeNode\n",
    "total_nclasses = len(np.unique(y))\n",
    "class TreeNode:\n",
    "    def __init__(self, X, feature_idx, feature_val, label_probs, information_gain, parent = None):\n",
    "        self.X = X\n",
    "        self.feature_idx: int = feature_idx\n",
    "        self.feature_val: float = feature_val\n",
    "        self.label_probs: list = label_probs\n",
    "        self.information_gain: float = information_gain\n",
    "        self.parent: TreeNode = parent\n",
    "        self.left: TreeNode = None\n",
    "        self.right: TreeNode = None\n",
    "    \n",
    "    def update_left(self, left):\n",
    "        self.left = left\n",
    "\n",
    "    def update_right(self, right):\n",
    "        self.right = right\n",
    "\n",
    "def get_data_prob(y):\n",
    "    \"\"\"Get the probability of each class in the given data\n",
    "    Args:\n",
    "        y (np.array): The target labels\n",
    "    Returns:\n",
    "        label_probs (np.array): The probability of each class in the given data\"\"\"\n",
    "    label_probs = np.zeros(total_nclasses, dtype = float)\n",
    "    for label in range(total_nclasses):\n",
    "        label_probs[label] = np.sum(y == label) / len(y)\n",
    "    return label_probs\n",
    "\n",
    "\n",
    "def build_tree(X, y, criterion, current_depth, max_depth=6, min_samples_leaf=1, parent = None):\n",
    "    \"\"\"Build a decision tree for the given data\n",
    "    Args:\n",
    "        X (np.array): The input data\n",
    "        criterion (str): The criterion to use for finding the best split\n",
    "        max_depth (int): The maximum depth of the tree\n",
    "    Returns:\n",
    "        root (TreeNode): The root node of the decision tree\"\"\"\n",
    "    \n",
    "    if current_depth > max_depth:\n",
    "        return None\n",
    "    \n",
    "    feature_idx, feature_val, best_score, x1, x2, y1, y2 = find_best_split(X, y, criterion)\n",
    "    label_probs = get_data_prob(y)\n",
    "    if criterion == 'entropy':\n",
    "        node_info = entropy(label_probs)\n",
    "    elif criterion == 'gini':\n",
    "        node_info = gini(label_probs)\n",
    "    information_gain = node_info - best_score\n",
    "    node = TreeNode(X, feature_idx, feature_val, label_probs, information_gain, parent)\n",
    "    \n",
    "    if len(x1) < min_samples_leaf or len(x2) < min_samples_leaf:\n",
    "        return node\n",
    "    #print(current_depth)\n",
    "    current_depth += 1\n",
    "    node.update_left(build_tree(x1, y1, criterion, current_depth, max_depth, parent = node))\n",
    "    node.update_right(build_tree(x2, y2, criterion, current_depth, max_depth, parent = node))\n",
    "\n",
    "    return node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = build_tree(X, y, 'entropy', 0, 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_feature_importance(node):\n",
    "    \"\"\"Calculate the feature importance of the tree\n",
    "    Args:\n",
    "        node (TreeNode): The root node of the tree\n",
    "    Returns:\n",
    "        feature_importances (dict): The feature importance of the tree\n",
    "    \"\"\"\n",
    "    if node is None:\n",
    "        return\n",
    "    if node.left is None and node.right is None:\n",
    "        return\n",
    "    if node.left is not None:\n",
    "        feature_importances[node.feature_idx] += node.information_gain\n",
    "        calculate_feature_importance(node.left)\n",
    "    if node.right is not None:\n",
    "        feature_importances[node.feature_idx] += node.information_gain\n",
    "        calculate_feature_importance(node.right)\n",
    "    return feature_importances\n",
    "feature_importances = dict.fromkeys(range(X.shape[1]), 0)\n",
    "feature_importances = calculate_feature_importance(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2 < 3.0\n",
      " |  - 0 < 4.4\n",
      " |  |  - 0 < 4.3\n",
      "       | - class:  0\n",
      " |  |  - 0 >= 4.3\n",
      "       | - class:  0\n",
      " |  - 0 >= 4.4\n",
      " |  |  - 0 < 4.5\n",
      " |  |  |  - 1 < 3.0\n",
      "          | - class:  0\n",
      " |  |  |  - 1 >= 3.0\n",
      "          | - class:  0\n",
      " |  |  - 0 >= 4.5\n",
      " |  |  |  - 0 < 4.6\n",
      "          | - class:  0\n",
      " |  |  |  - 0 >= 4.6\n",
      "          | - class:  0\n",
      " - 2 >= 3.0\n",
      " |  - 3 < 1.8\n",
      " |  |  - 2 < 5.6\n",
      " |  |  |  - 0 < 4.9\n",
      "          | - class:  1\n",
      " |  |  |  - 0 >= 4.9\n",
      "          | - class:  1\n",
      " |  |  - 2 >= 5.6\n",
      " |  |  |  - 0 < 6.1\n",
      "          | - class:  2\n",
      " |  |  |  - 0 >= 6.1\n",
      "          | - class:  2\n",
      " |  - 3 >= 1.8\n",
      " |  |  - 0 < 5.6\n",
      "       | - class:  2\n",
      " |  |  - 0 >= 5.6\n",
      "       | - class:  2\n"
     ]
    }
   ],
   "source": [
    "Node = tree\n",
    "\n",
    "def print_tree(Node, depth = 0):\n",
    "    if Node is None:\n",
    "        return\n",
    "    print(' | '*depth,'-', Node.feature_idx, '<' , Node.feature_val)\n",
    "    if Node.left is None and Node.right is None:\n",
    "        print('   '*depth, '| - class: ', np.argmax(Node.label_probs))\n",
    "    print_tree(Node.left, depth+1)\n",
    "    print(' | '*depth,'-', Node.feature_idx, '>=' , Node.feature_val)\n",
    "    if Node.left is None and Node.right is None:\n",
    "        print('   '*depth, '| - class: ', np.argmax(Node.label_probs))\n",
    "    print_tree(Node.right, depth+1)\n",
    "\n",
    "print_tree(Node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X: np.array) -> np.array:\n",
    "    \"\"\"Predict the class of the given input data\n",
    "    Args:\n",
    "        X (np.array): The input data\n",
    "    Returns:\n",
    "        predictions (np.array): The predicted class of the input data\"\"\"\n",
    "    predictions = []\n",
    "    for x in X:\n",
    "        node = tree\n",
    "        while node.left is not None and node.right is not None:\n",
    "            if x[node.feature_idx] < node.feature_val:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        predictions.append(np.argmax(node.label_probs))\n",
    "    return np.array(predictions)\n",
    "y_pred = predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97 %\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(y, y_pred)\n",
    "\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\": 10})\n",
    "\n",
    "\n",
    "acc = np.sum(y==y_pred)/len(y_pred)\n",
    "print(\"Accuracy: {} %\".format(round(acc*100),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 93 %\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "tree = build_tree(X_train, y_train, 'entropy', 0, 5, 3)\n",
    "y_pred = predict(X_test)\n",
    "\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\": 10})\n",
    "\n",
    "\n",
    "acc = np.sum(y_test==y_pred)/len(y_pred)\n",
    "print(\"Accuracy: {} %\".format(round(acc*100),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class ClassificationTreeNode:\n",
    "    def __init__(self, X, feature_idx, feature_val, label_probs, information_gain, parent = None):\n",
    "        self.X = X\n",
    "        self.feature_idx: int = feature_idx\n",
    "        self.feature_val: float = feature_val\n",
    "        self.label_probs: list = label_probs\n",
    "        self.information_gain: float = information_gain\n",
    "        self.parent: ClassificationTreeNode = parent\n",
    "        self.left: ClassificationTreeNode = None\n",
    "        self.right: ClassificationTreeNode = None\n",
    "    \n",
    "    def update_left(self, left):\n",
    "        self.left = left\n",
    "\n",
    "    def update_right(self, right):\n",
    "        self.right = right\n",
    "\n",
    "class DecisionTreeClassifier:\n",
    "    def __init__(self, criterion: str = 'entropy', max_depth: int = 6, min_samples_leaf: int = 1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            criterion (str): criterion to split the data\n",
    "            max_depth (int): maximum depth of the tree\n",
    "            min_samples_leaf (int): minimum number of samples required to be at a leaf node\n",
    "        \"\"\"\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.tree: DecisionTreeClassifier = None\n",
    "\n",
    "    def update_left(self, left):\n",
    "        self.left = left\n",
    "\n",
    "    def update_right(self, right):\n",
    "        self.right = right\n",
    "\n",
    "    def entropy(self, class_probabilitues: list) -> float:\n",
    "        \"\"\"Implement the entropy function\n",
    "        Args:\n",
    "            class_probabilitues (list): A list of class probabilities\n",
    "        Returns:\n",
    "            entropy (float): The entropy of the given class probabilities\"\"\"\n",
    "        return sum([-p * np.log2(p+1e-10) for p in class_probabilitues])\n",
    "\n",
    "    def gini(self, class_probabilities: list) -> float:\n",
    "        \"\"\"Implement the gini function\n",
    "        Args:\n",
    "            class_probabilities (list): A list of class probabilities\n",
    "        Returns:\n",
    "            gini (float): The gini of the given class probabilities\"\"\"\n",
    "        return 1 - sum([p**2 for p in class_probabilities])\n",
    "    \n",
    "    def split(self, X: np.array, y: np.array, feature_idx: int, feature_val: float) -> Tuple[np.array, np.array]:\n",
    "        \"\"\"Split the data into two group with the given feature and value\n",
    "        Args:\n",
    "            X (np.array): The input data\n",
    "            y (np.array): The target labels\n",
    "            feature_idx (int): The index of the feature to split\n",
    "            feature_val (float): The value to split on\n",
    "        Returns:\n",
    "            x1 (np.array): left data\n",
    "            x2 (np.array): right data\n",
    "            y1 (np.array): left labels\n",
    "            y2 (np.array): right labels\n",
    "            p1 (np.array): The probability of each class in the left split data\n",
    "            p2 (np.array): The probability of each class in the right split data\"\"\"\n",
    "        x1 = X[X.T[feature_idx]<feature_val]\n",
    "        x2 = X[X.T[feature_idx]>=feature_val]\n",
    "        y1 = y[X.T[feature_idx]<feature_val]\n",
    "        y2 = y[X.T[feature_idx]>=feature_val]\n",
    "        n_classes = len(np.unique(y))\n",
    "\n",
    "        if len(y1) == 0:\n",
    "            p1 = [0]*n_classes\n",
    "        else:\n",
    "            p1 = [np.sum(y1==c)/len(y1) for c in range(n_classes)]\n",
    "        if len(y2) == 0:\n",
    "            p2 = [0]*n_classes\n",
    "        else:\n",
    "            p2 = [np.sum(y2==c)/len(y2) for c in range(n_classes)]\n",
    "\n",
    "        return x1, x2, y1, y2, p1, p2\n",
    "\n",
    "    def find_best_split(self, X: np.array, y: np.array) -> Tuple[int, float, float, np.array, np.array, np.array, np.array]:\n",
    "        \"\"\"Find the best split for the given data and criterion\n",
    "        Args:\n",
    "            X (np.array): The input data\n",
    "            y (np.array): The target labels\n",
    "        Returns:\n",
    "            feature_idx (int): The index of the feature to split\n",
    "            feature_val (float): The value to split on\n",
    "            best_score (float): The best score of the split\n",
    "            x1 (np.array): The left split data\n",
    "            x2 (np.array): The right split data\n",
    "            y1 (np.array): The left split target labels\n",
    "            y2 (np.array): The right split target labels\n",
    "            \"\"\"\n",
    "        n_features = X.shape[1]\n",
    "        best_score = 10e+10\n",
    "        flag = False\n",
    "        for feature_idx in range(n_features):\n",
    "            for feature_val in np.unique(X.T[feature_idx]):\n",
    "                x1, x2, y1, y2, p1, p2 = self.split(X, y, feature_idx, feature_val)\n",
    "                current_split = {'feature_idx': feature_idx, 'feature_val': feature_val, \n",
    "                                'x1': x1, 'x2': x2, 'y1': y1, 'y2': y2, 'best_score': best_score, 'p1': p1, 'p2': p2}\n",
    "                if self.criterion == 'entropy':\n",
    "                    score = self.entropy(p1) + self.entropy(p2)\n",
    "                elif self.criterion == 'gini':\n",
    "                    score = self.gini(p1) + self.gini(p2)\n",
    "                if score < best_score:\n",
    "                    flag = True\n",
    "                    best_score = score\n",
    "                    best_split = current_split\n",
    "        if flag:\n",
    "            return best_split['feature_idx'], best_split['feature_val'], best_split['best_score'], \\\n",
    "                    best_split['x1'], best_split['x2'], best_split['y1'], best_split['y2']\n",
    "        else:\n",
    "            return current_split['feature_idx'], current_split['feature_val'], current_split['best_score'], \\\n",
    "                    current_split['x1'], current_split['x2'], current_split['y1'], current_split['y2']\n",
    "            \n",
    "    def get_data_prob(self, y: np.array) -> np.array:\n",
    "        \"\"\"Get the probability of each class in the given data\n",
    "        Args:\n",
    "            y (np.array): The target labels\n",
    "        Returns:\n",
    "            label_probs (np.array): The probability of each class in the given data\"\"\"\n",
    "        label_probs = np.zeros(self.total_nclasses, dtype = float)\n",
    "        for label in range(self.total_nclasses):\n",
    "            label_probs[label] = np.sum(y == label) / len(y)\n",
    "        return label_probs\n",
    "\n",
    "    def fit(self, X: np.array, y: np.array, current_depth: int = 0, parent = None) -> ClassificationTreeNode:\n",
    "        \"\"\"Build a decision tree for the given data\n",
    "        Args:\n",
    "            X (np.array): The input data\n",
    "            max_depth (int): The maximum depth of the tree\n",
    "        Returns:\n",
    "            root (ClassificationTreeNode): The root node of the decision tree\"\"\"\n",
    "        \n",
    "        if current_depth == 0:\n",
    "            self.total_nclasses = len(np.unique(y))\n",
    "\n",
    "        if current_depth > self.max_depth:\n",
    "            return None\n",
    "        \n",
    "        feature_idx, feature_val, best_score, x1, x2, y1, y2 = self.find_best_split(X, y)\n",
    "        label_probs = self.get_data_prob(y)\n",
    "        if self.criterion == 'entropy':\n",
    "            node_info = self.entropy(label_probs)\n",
    "        elif self.criterion == 'gini':\n",
    "            node_info = self.gini(label_probs)\n",
    "        information_gain = node_info - best_score\n",
    "        tree = ClassificationTreeNode(X, feature_idx, feature_val, label_probs, information_gain, parent)\n",
    "\n",
    "        if len(x1) < self.min_samples_leaf or len(x2) < self.min_samples_leaf:\n",
    "            self.tree = tree\n",
    "            return self.tree\n",
    "        #print(current_depth)\n",
    "        current_depth += 1\n",
    "        tree.left = (self.fit(x1, y1, current_depth, parent = tree))\n",
    "        tree.right = (self.fit(x2, y2, current_depth, parent = tree))\n",
    "\n",
    "        self.tree = tree\n",
    "        return self.tree\n",
    "    \n",
    "    def predict(self, X: np.array) -> np.array:\n",
    "        \"\"\"Predict the class of the given input data\n",
    "        Args:\n",
    "            X (np.array): The input data\n",
    "        Returns:\n",
    "            predictions (np.array): The predicted class of the input data\"\"\"\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            node = self.tree\n",
    "            while node.left is not None and node.right is not None:\n",
    "                if x[node.feature_idx] < node.feature_val:\n",
    "                    node = node.left\n",
    "                else:\n",
    "                    node = node.right\n",
    "            predictions.append(np.argmax(node.label_probs))\n",
    "        return np.array(predictions)\n",
    "    \n",
    "    def print_tree(self, Node: ClassificationTreeNode, depth: int = 0):\n",
    "        \"\"\"Print the decision tree\n",
    "        Args:\n",
    "            Node (ClassificationTreeNode): The root node of the tree\n",
    "            depth (int): The depth of the tree\"\"\"\n",
    "        if depth == 0:\n",
    "            Node = self.tree\n",
    "        if Node is None:\n",
    "            return\n",
    "        print(' | '*depth,'-', Node.feature_idx, '<' , Node.feature_val)\n",
    "        if Node.left is None and Node.right is None:\n",
    "            print('   '*depth, '| - class :', np.argmax(Node.label_probs))\n",
    "        self.print_tree(Node.left, depth+1)\n",
    "        print(' | '*depth,'-', Node.feature_idx, '>=' , Node.feature_val)\n",
    "        if Node.left is None and Node.right is None:\n",
    "            print('   '*depth, '| - class: ', np.argmax(Node.label_probs))\n",
    "        self.print_tree(Node.right, depth+1)\n",
    "\n",
    "    def calculate_feature_importance(self, node):\n",
    "        \"\"\"Calculate the feature importance of the tree\n",
    "        Args:\n",
    "            node (TreeNode): The root node of the tree\n",
    "        Returns:\n",
    "            feature_importances (dict): The feature importance of the tree\n",
    "        \"\"\"\n",
    "        if node is None:\n",
    "            return\n",
    "        if node.left is None and node.right is None:\n",
    "            return\n",
    "        if node.left is not None:\n",
    "            self.feature_importances[node.feature_idx] += node.information_gain\n",
    "            self.calculate_feature_importance(node.left)\n",
    "        if node.right is not None:\n",
    "            self.feature_importances[node.feature_idx] += node.information_gain\n",
    "            self.calculate_feature_importance(node.right)\n",
    "        return self.feature_importances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2 < 3.3\n",
      " |  - 0 < 4.4\n",
      " |  |  - 0 < 4.3\n",
      "       | - class : 0\n",
      " |  |  - 0 >= 4.3\n",
      "       | - class:  0\n",
      " |  - 0 >= 4.4\n",
      " |  |  - 0 < 4.5\n",
      " |  |  |  - 1 < 3.0\n",
      "          | - class : 0\n",
      " |  |  |  - 1 >= 3.0\n",
      "          | - class:  0\n",
      " |  |  - 0 >= 4.5\n",
      " |  |  |  - 0 < 4.6\n",
      "          | - class : 0\n",
      " |  |  |  - 0 >= 4.6\n",
      "          | - class:  0\n",
      " - 2 >= 3.3\n",
      " |  - 3 < 1.7\n",
      " |  |  - 2 < 5.6\n",
      " |  |  |  - 0 < 4.9\n",
      "          | - class : 1\n",
      " |  |  |  - 0 >= 4.9\n",
      "          | - class:  1\n",
      " |  |  - 2 >= 5.6\n",
      " |  |  |  - 0 < 6.1\n",
      "          | - class : 2\n",
      " |  |  |  - 0 >= 6.1\n",
      "          | - class:  2\n",
      " |  - 3 >= 1.7\n",
      " |  |  - 0 < 4.9\n",
      "       | - class : 2\n",
      " |  |  - 0 >= 4.9\n",
      "       | - class:  2\n",
      "Accuracy: 93 %\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "\n",
    "# load iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X=np.array(iris.data)\n",
    "y = np.array(iris.target)\n",
    "\n",
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Build up the decision tree classifier\n",
    "dt = DecisionTreeClassifier(max_depth = 3)\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Predict on testing data\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "# Print out the result\n",
    "dt.print_tree(dt.tree)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\": 10})\n",
    "\n",
    "\n",
    "acc = np.sum(y_test==y_pred)/len(y_pred)\n",
    "print(\"Accuracy: {} %\".format(round(acc*100),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree can also be apply to regression probelms, the differnce is that MSE is used to determine the best split,\n",
    "\n",
    "$$MSE = \\frac{1}{n} \\Sigma_i^n {(y_i-\\overline{y})^2}$$\n",
    "\n",
    "where $\\overline{y}$ is the mean of y in the group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class RegressionTreeNode:\n",
    "    def __init__(self, X, feature_idx, feature_val, parent = None):\n",
    "        self.X = X\n",
    "        self.feature_idx: int = feature_idx\n",
    "        self.feature_val: float = feature_val\n",
    "        self.parent: RegressionTreeNode = parent\n",
    "        self.left: RegressionTreeNode = None\n",
    "        self.right: RegressionTreeNode = None\n",
    "        self.predict_val: float = None\n",
    "    \n",
    "    def update_left(self, left):\n",
    "        self.left = left\n",
    "\n",
    "    def update_right(self, right):\n",
    "        self.right = right\n",
    "\n",
    "class DecisionTreeRegressor:\n",
    "    def __init__(self, max_depth: int = 6, min_samples_leaf: int = 1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            max_depth (int): maximum depth of the tree\n",
    "            min_samples_leaf (int): minimum number of samples required to be at a leaf node\n",
    "        \"\"\"\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.tree: DecisionTreeRegressor = None\n",
    "\n",
    "    def mse(self, y_left: np.array, y_right: np.array) -> Tuple[float, float]:\n",
    "        \"\"\"Implement the mse function\n",
    "        Args:\n",
    "            y_left (np.array): An array of y values in left group\n",
    "            y_right (np.array): An array of y values in right group\n",
    "        Returns:\n",
    "            mes_left (float): The mse of the given y_left\n",
    "            mes_right (float): The mse of the given y_right\"\"\"\n",
    "        mes_left = np.mean([(y_left[i]-np.mean(y_left))**2 for i in range(len(y_left))])\n",
    "        mes_right = np.mean([(y_right[i]-np.mean(y_right))**2 for i in range(len(y_right))])\n",
    "        return mes_left, mes_right\n",
    "    \n",
    "    def split(self, X: np.array, y: np.array, feature_idx: int, feature_val: float) -> Tuple[np.array, np.array]:\n",
    "        \"\"\"Split the data into two group with the given feature and value\n",
    "        Args:\n",
    "            X (np.array): The input data\n",
    "            y (np.array): The target labels\n",
    "            feature_idx (int): The index of the feature to split\n",
    "            feature_val (float): The value to split on\n",
    "        Returns:\n",
    "                X_left (np.array): The probability of each class in the left split data\n",
    "                X_right (np.array): The probability of each class in the right split data\"\"\"\n",
    "        x1 = X[X.T[feature_idx]<feature_val]\n",
    "        x2 = X[X.T[feature_idx]>=feature_val]\n",
    "        y1 = y[X.T[feature_idx]<feature_val]\n",
    "        y2 = y[X.T[feature_idx]>=feature_val]\n",
    "\n",
    "        return x1, x2, y1, y2\n",
    "\n",
    "\n",
    "    def get_cost(self, y1, y2):\n",
    "        \"\"\"Calculate the cost of the split\n",
    "        Args:\n",
    "            y1 (np.array): The target labels in the left split\n",
    "            y2 (np.array): The target labels in the right split\n",
    "        Returns:\n",
    "            cost (float): The cost of the split\"\"\"\n",
    "        mse_left, mse_right = self.mse(y1, y2)\n",
    "        return (len(y1)/(len(y1)+len(y2))) * mse_left + (len(y2)/(len(y1)+len(y2))) * mse_right\n",
    "\n",
    "\n",
    "    def find_best_split(self, X: np.array, y: np.array) -> Tuple[int, float, float, np.array, np.array, np.array, np.array]:\n",
    "        \"\"\"Find the best split for the given data and criterion\n",
    "        Args:\n",
    "            X (np.array): The input data\n",
    "            y (np.array): The target labels\n",
    "        Returns:\n",
    "            feature_idx (int): The index of the feature to split\n",
    "            feature_val (float): The value to split on\n",
    "            best_score (float): The best score of the split\n",
    "            x1 (np.array): The left split data\n",
    "            x2 (np.array): The right split data\n",
    "            y1 (np.array): The left split target labels\n",
    "            y2 (np.array): The right split target labels\n",
    "            \"\"\"\n",
    "        n_features = X.shape[1]\n",
    "        best_score = 10e+10\n",
    "        flag = False\n",
    "        for feature_idx in range(n_features):\n",
    "            all_val = np.unique(X.T[feature_idx])\n",
    "            val = np.unique([np.percentile(all_val, 20),np.percentile(all_val, 40),np.percentile(all_val, 60),\n",
    "                             np.percentile(all_val, 80)])\n",
    "            for feature_val in val:\n",
    "                x1, x2, y1, y2 = self.split(X, y, feature_idx, feature_val)\n",
    "                current_split = {'feature_idx': feature_idx, 'feature_val': feature_val, \n",
    "                                'x1': x1, 'x2': x2, 'y1': y1, 'y2': y2, 'best_score': best_score}\n",
    "                score = self.get_cost(y1, y2)\n",
    "                if score < best_score:\n",
    "                    flag = True\n",
    "                    best_score = score\n",
    "                    best_split = current_split\n",
    "        if flag:\n",
    "            return best_split['feature_idx'], best_split['feature_val'], best_split['best_score'], \\\n",
    "                    best_split['x1'], best_split['x2'], best_split['y1'], best_split['y2']\n",
    "        else:\n",
    "            return current_split['feature_idx'], current_split['feature_val'], current_split['best_score'], \\\n",
    "                    current_split['x1'], current_split['x2'], current_split['y1'], current_split['y2']\n",
    "    \n",
    "\n",
    "    def fit(self, X: np.array, y: np.array, current_depth: int = 0, parent = None) -> RegressionTreeNode:\n",
    "        \"\"\"Build a decision tree for the given data\n",
    "        Args:\n",
    "            X (np.array): The input data\n",
    "            max_depth (int): The maximum depth of the tree\n",
    "        Returns:\n",
    "            root (RegressionTreeNode): The root node of the decision tree\"\"\"\n",
    "        \n",
    "        self.total_nclasses = len(np.unique(y))\n",
    "        \n",
    "\n",
    "        if current_depth > self.max_depth:\n",
    "            return None\n",
    "        \n",
    "        feature_idx, feature_val, best_score, x1, x2, y1, y2 = self.find_best_split(X, y)\n",
    "        tree = RegressionTreeNode(X, feature_idx, feature_val, parent)\n",
    "        tree.predict_values = np.mean(y)\n",
    "        \n",
    "        if len(x1) < self.min_samples_leaf or len(x2) < self.min_samples_leaf:\n",
    "            self.tree = tree\n",
    "            return self.tree\n",
    "        #print(current_depth)\n",
    "        current_depth += 1\n",
    "        tree.left = (self.fit(x1, y1, current_depth, parent = tree))\n",
    "        tree.right = (self.fit(x2, y2, current_depth, parent = tree))\n",
    "\n",
    "        self.tree = tree\n",
    "        return self.tree\n",
    "    \n",
    "    def predict(self, X: np.array) -> np.array:\n",
    "        \"\"\"Predict the class of the given input data\n",
    "        Args:\n",
    "            X (np.array): The input data\n",
    "        Returns:\n",
    "            predictions (np.array): The predicted class of the input data\"\"\"\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            node = self.tree\n",
    "            while node.left is not None and node.right is not None:\n",
    "                if x[node.feature_idx] < node.feature_val:\n",
    "                    node = node.left\n",
    "                else:\n",
    "                    node = node.right\n",
    "            predictions.append(node.predict_values)\n",
    "        return np.array(predictions)\n",
    "    \n",
    "    def print_tree(self, Node: RegressionTreeNode, depth: int = 0):\n",
    "        \"\"\"Print the decision tree\n",
    "        Args:\n",
    "            Node (RegressionTreeNode): The root node of the tree\n",
    "            depth (int): The depth of the tree\"\"\"\n",
    "        if depth == 0:\n",
    "            Node = self.tree\n",
    "        if Node is None:\n",
    "            return\n",
    "        print(' | '*depth,'-', Node.feature_idx, '<' , Node.predict_values)\n",
    "        if Node.left is None and Node.right is None:\n",
    "            print('   '*depth, '| - value :', np.argmax(Node.predict_values))\n",
    "        self.print_tree(Node.left, depth+1)\n",
    "        print(' | '*depth,'-', Node.feature_idx, '>=' , Node.predict_values)\n",
    "        if Node.left is None and Node.right is None:\n",
    "            print('   '*depth, '| - value: ', np.argmax(Node.predict_values))\n",
    "        self.print_tree(Node.right, depth+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 8 < 153.37677053824362\n",
      " |  - 2 < 118.02564102564102\n",
      " |  |  - 8 < 99.41666666666667\n",
      " |  |  |  - 0 < 82.15686274509804\n",
      "          | - value : 0\n",
      " |  |  |  - 0 >= 82.15686274509804\n",
      "          | - value:  0\n",
      " |  |  - 8 >= 99.41666666666667\n",
      " |  |  |  - 2 < 110.28395061728395\n",
      "          | - value : 0\n",
      " |  |  |  - 2 >= 110.28395061728395\n",
      "          | - value:  0\n",
      " |  - 2 >= 118.02564102564102\n",
      " |  |  - 6 < 157.015873015873\n",
      " |  |  |  - 9 < 194.96\n",
      "          | - value : 0\n",
      " |  |  |  - 9 >= 194.96\n",
      "          | - value:  0\n",
      " |  |  - 6 >= 157.015873015873\n",
      " |  |  |  - 2 < 132.05263157894737\n",
      "          | - value : 0\n",
      " |  |  |  - 2 >= 132.05263157894737\n",
      "          | - value:  0\n",
      " - 8 >= 153.37677053824362\n",
      " |  - 2 < 197.00632911392404\n",
      " |  |  - 7 < 158.40579710144928\n",
      " |  |  |  - 8 < 132.59259259259258\n",
      "          | - value : 0\n",
      " |  |  |  - 8 >= 132.59259259259258\n",
      "          | - value:  0\n",
      " |  |  - 7 >= 158.40579710144928\n",
      " |  |  |  - 2 < 175.0\n",
      "          | - value : 0\n",
      " |  |  |  - 2 >= 175.0\n",
      "          | - value:  0\n",
      " |  - 2 >= 197.00632911392404\n",
      " |  |  - 3 < 226.93258426966293\n",
      " |  |  |  - 8 < 203.6122448979592\n",
      "          | - value : 0\n",
      " |  |  |  - 8 >= 203.6122448979592\n",
      "          | - value:  0\n",
      " |  |  - 3 >= 226.93258426966293\n",
      " |  |  |  - 3 < 255.5\n",
      "          | - value : 0\n",
      " |  |  |  - 3 >= 255.5\n",
      "          | - value:  0\n",
      "RMSE:  64.9118055495998\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "diabate = load_diabetes()\n",
    "X = diabate.data\n",
    "y = diabate.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "dtr = DecisionTreeRegressor(max_depth = 3)\n",
    "\n",
    "rt = dtr.fit(X_train, y_train)\n",
    "\n",
    "dtr.print_tree(rt)\n",
    "\n",
    "y_pred = dtr.predict(X_test)\n",
    "\n",
    "print('RMSE: ', np.sqrt(np.mean((y_test - y_pred) ** 2 )))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reference\n",
    "\n",
    "https://medium.com/@enozeren/building-a-decision-tree-from-scratch-324b9a5ed836\n",
    "\n",
    "https://github.com/zotroneneis/machine_learning_basics/blob/master/decision_tree_regression.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
